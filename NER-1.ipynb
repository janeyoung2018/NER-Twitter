{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER) on Twitter \n",
    "\n",
    "In this notewook, I will use 4 ways solve custom Named Entity Recognition (NER) problem on Twitter. NER is a task that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "In this dataset, we have 21 different tags for sentences.\n",
    "\n",
    "tags = ['O', 'B-musicartist', 'I-musicartist', 'B-product', 'I-product', 'B-company', 'B-person', 'B-other', 'I-other', 'B-facility',\n",
    "    'I-facility', 'B-sportsteam', 'B-geo-loc', 'I-geo-loc', 'I-company', 'I-person', 'B-movie', 'I-movie', 'B-tvshow', 'I-tvshow',\n",
    "    'I-sportsteam'],\n",
    "\n",
    "where 'B-' and 'I-' prefixes stand for the beginning and inside of the entity, 'O' stands for out of tag or no tag.\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "In the following three notebooks, we will use five ways to examine the dataset.\n",
    "\n",
    "- <mark>Naive Bayes multinomial model</mark>\n",
    "- <mark>Conditional Random Fields (CRFs)</mark>\n",
    "- <mark>Custom SpaCy</mark>\n",
    "- BERT in Spark NLP\n",
    "- Simple Transformer \n",
    "\n",
    "In this notebook we will only consider the first three model. In the following two notbooks we will discuss the last two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            ######################################\n",
    "            ######### YOUR CODE HERE #############\n",
    "            ######################################\n",
    "            if token[0] == \"@\":\n",
    "                token = \"<USR>\"\n",
    "            elif token[:7] == \"http://\" or token[:8] == \"https://\":\n",
    "                token = \"<URL>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes classifier for multinomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform list data to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tokens = pd.DataFrame({'words':train_tokens})\n",
    "df_train_tokens = df_train_tokens.explode('words')\n",
    "df_train_tokens[\"sentence #\"] = df_train_tokens.index\n",
    "df_train_tokens = df_train_tokens.reset_index(drop=True)\n",
    "\n",
    "df_train_tags = pd.DataFrame({'tags':train_tags})\n",
    "df_train_tags = df_train_tags.explode('tags').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tokens = pd.DataFrame({'words':test_tokens})\n",
    "df_test_tokens = df_test_tokens.explode('words')\n",
    "df_test_tokens[\"sentence #\"] = df_test_tokens.index\n",
    "df_test_tokens = df_test_tokens.reset_index(drop=True)\n",
    "\n",
    "df_test_tags = pd.DataFrame({'tags':test_tags})\n",
    "df_test_tags = df_test_tags.explode('tags').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of tags(labels)\n",
    "- Class \"O\" is highly represented, 1670 times of the counts of \"B-tvshow\". Thus the data is highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_counts_train = df_train_tags.tags.value_counts()\n",
    "df_value_counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_counts_test = df_test_tags.tags.value_counts()\n",
    "df_value_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_percentage_train = df_value_counts_train / float(df_train_tags.shape[0])\n",
    "df_value_percentage_test = df_value_counts_test / float(df_test_tags.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_percentage_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_percentage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distr = pd.DataFrame(df_value_percentage_train)\n",
    "df_distr.columns = ['Train']\n",
    "df_distr[\"Test\"] = df_value_percentage_test\n",
    "df_distr = df.drop(df.index[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_distr = df.plot.bar(figsize=(10,5))\n",
    "fig_distr.figure.savefig('./images/distribution.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_value_counts_train.plot.bar(figsize=(10,5))\n",
    "ax.figure.savefig('./images/counts.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the train data to vector using DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "X_train_nb = v.fit_transform(df_train_tokens.to_dict('records'))\n",
    "y_train_nb = df_train_tags.tags.values\n",
    "classes = np.unique(y_train_nb)\n",
    "classes = classes.tolist()\n",
    "\n",
    "X_train_nb.shape, y_train_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_nb = v.transform(df_test_tokens.to_dict('records'))\n",
    "y_test_nb = df_test_tags.tags.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag \"O\" is highly represented. Remove \"O\" when evaluate metrics precision, recall and f1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = classes.copy()\n",
    "new_classes = new_classes[:-1]\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV to evaluating estimator performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "nb = MultinomialNB()\n",
    "params_space = { 'alpha': [0.01, 0.1, 1.0]}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.f1_score,\n",
    "                        average='macro', labels=new_classes)\n",
    "\n",
    "# search\n",
    "gs = GridSearchCV(nb, params_space,\n",
    "                        cv=5,\n",
    "                        verbose=1,\n",
    "                        n_jobs=3,\n",
    "                        scoring=f1_scorer)\n",
    "gs.fit(X_train_nb, y_train_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print best parameters and best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best params:', gs.best_params_)\n",
    "print('best CV score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "print(f1_score(y_pred=nb.predict(X_train_nb), y_true=y_train_nb, labels=classes, average='micro'))\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "print(f1_score(y_pred=nb.predict(X_test_nb), y_true=y_test_nb, labels=classes, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "print(classification_report(y_pred=nb.predict(X_train_nb), y_true=y_train_nb, labels=new_classes))\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "print(classification_report(y_pred=nb.predict(X_test_nb), y_true=y_test_nb, labels=new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conditional Random Fields (CRFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data in CRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_tokens\n",
    "df_train['tags'] = df_train_tags[\"tags\"]\n",
    "\n",
    "\n",
    "df_test = df_test_tokens\n",
    "df_test['tags'] = df_test_tags[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['words'].values.tolist(),\n",
    "                                                           s['tags'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter_train = SentenceGetter(df_train)\n",
    "sentences_train = getter_train.sentences\n",
    "\n",
    "\n",
    "getter_test = SentenceGetter(df_test)\n",
    "sentences_test = getter_test.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token,  label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_crf = [sent2features(s) for s in sentences_train]\n",
    "y_train_crf = [sent2labels(s) for s in sentences_train]\n",
    "\n",
    "\n",
    "X_test_crf = [sent2features(s) for s in sentences_test]\n",
    "y_test_crf = [sent2labels(s) for s in sentences_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='macro', labels=classes)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-2,\n",
    "                        n_iter=30,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train_crf, y_train_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "print(metrics.flat_f1_score(y_pred=crf.predict(X_train_crf), y_true=y_train_crf, labels=classes, average='micro'))\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "print(metrics.flat_f1_score(y_pred=crf.predict(X_test_crf), y_true=y_test_crf, labels=classes, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "print(metrics.flat_classification_report(y_pred=crf.predict(X_train_crf), y_true=y_train_crf, labels=new_classes))\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "print(metrics.flat_classification_report(y_pred=crf.predict(X_test_crf), y_true=y_test_crf, labels=new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRFs Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eli5 check weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(crf, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(crf, top=10, targets=['O', 'B-company', 'I-person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(crf, top=10, feature_re='^word\\.is',\n",
    "                  horizontal_layout=False, show=['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SpaCy\n",
    "\n",
    "- Oneline learning of pre-trained spacy ner model.\n",
    "\n",
    "Architecture of spacy ner:\n",
    "\n",
    "- The Spacy NER system contains a word embedding strategy using <mark>sub word features</mark> and <mark>\"Bloom\" embed</mark>, and a deep <mark>convolution</mark> neural network with <mark>residual</mark> connections(<mark>residual CNNs</mark>). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data in SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(train_tokens)):\n",
    "    text = \" \".join(train_tokens[i])\n",
    "    entities = []\n",
    "    token_start_point = 0\n",
    "    for j in range(len(train_tags[i])):\n",
    "        entities.append((token_start_point, token_start_point + len(train_tokens[i][j]) ,train_tags[i][j].upper()))\n",
    "        token_start_point += len(train_tokens[i][j]) + 1\n",
    "    train_data.append((text, {\"entities\" : entities}))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train_data\", 'wb') as fp:\n",
    "#     pickle.dump(train_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline and entity recognizer.\n",
    "model = None\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new entity labels to entity recognizer\n",
    "\n",
    "LABEL = [item.upper() for item in classes]\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)\n",
    "# Inititalizing optimizer\n",
    "if model is None:\n",
    "    optimizer = nlp.begin_training()\n",
    "else:\n",
    "    optimizer = nlp.entity.create_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of other pipes to disable them during training to train # only NER and update the weights\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "n_iter = 30\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, \n",
    "                            size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch) \n",
    "            # Updating the weights\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print('Losses', losses)\n",
    "print(time.time()-start)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model \n",
    "# from pathlib import Path\n",
    "# new_model_name = \"spacy_ner\"\n",
    "# output_dir = '/Users/wenjuanyang/natural-language-processing/week2'\n",
    "# if output_dir is not None:\n",
    "#     output_dir = Path(output_dir)\n",
    "#     if not output_dir.exists():\n",
    "#         output_dir.mkdir()\n",
    "#     nlp.meta['name'] = new_model_name  # rename model\n",
    "#     nlp.to_disk(output_dir)\n",
    "#     print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- As spacy split sentence in different ways. len(y_train) != len(y_pred), so here we cann't use classification_report or flat_classification_report to evaluate the performance spacy nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot[\"entities\"])\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from dir\n",
    "#nlp2 = spacy.load(output_dir)\n",
    "#train_results = evaluate(nlp2, train_data)\n",
    "#print(train_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_results = evaluate(nlp, train_data)\n",
    "print(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(len(test_tokens)):\n",
    "    text = \" \".join(test_tokens[i])\n",
    "    entities = []\n",
    "    token_start_point = 0\n",
    "    for j in range(len(test_tags[i])):\n",
    "        entities.append((token_start_point, token_start_point + len(test_tokens[i][j]) ,test_tags[i][j].upper()))\n",
    "        token_start_point += len(test_tokens[i][j]) + 1\n",
    "    test_data.append((text, {\"entities\" : entities}))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate(nlp, test_data)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_score(results):\n",
    "    f1_score_sum = 0\n",
    "    for key, _ in results['ents_per_type'].items():\n",
    "        if key != 'O':\n",
    "            f1_score_sum += results['ents_per_type'][key]['f']\n",
    "    return f1_score_sum/(len(results['ents_per_type']) - 1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_macro_f1 = macro_f1_score(test_results)\n",
    "test_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_macro_f1 = macro_f1_score(train_results)\n",
    "train_macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrong Splits\n",
    "- Positive example: \"I just called into work Tuesday night...it 's\", split \"night...it\" into three seperate part.\n",
    "\n",
    "- Negative example: \"Twist Ring Twist Ring by* TheJewelryGirlsPlace\", treat \"by* TheJewelryGirlsPlace\" as subwords.\n",
    "  In some examples miss some words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_split(ner_model,test_data):\n",
    "    wrong_count = 0\n",
    "    for i in range(len(test_data)):\n",
    "        doc = ner_model(test_data[i][0])\n",
    "        pred = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        if len(pred) != len(test_data[i][1][\"entities\"]):\n",
    "            print(i, \"\\n\")\n",
    "            print(test_data[i][0], \"\\n\")\n",
    "            print(pred, \"\\n\")\n",
    "            print(test_data[i][1][\"entities\"], \"\\n\\n\")\n",
    "            wrong_count += 1\n",
    "        if wrong_count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_split(nlp, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr",
   "language": "python",
   "name": "dsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
